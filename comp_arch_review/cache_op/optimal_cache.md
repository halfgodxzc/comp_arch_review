# 优化缓存性能的不同方法与心得
## 第一种：减少L1 Cache大小，降低Hit time，降低功耗
### 基本逻辑
<p> 1：首先明确，cache Hit的三个步骤：首先是用index查到tag，对比tag和tlb中查到的tag，如果是set associative缓存，还需要使用多路复用器对比。 显然，降低相关度将减少多路复用器的设计成本，减小功耗（PS：为了潜在读者，以及以后的自己，解释一下相关度是什么，其实就是4-way set associative 或者 8-way...里面这个数字）。</p>
<p>同理，其实cache里对比index值的电路也希望index的位数少一点，这样同样可以降低功耗，那么在cache size不变的情况下，可以增加cache block的大小。但是这样会导致cache miss增加。（书中没有解释原因，个人认为是index位数变少，tag位数变多，区分度下降。）以上是相关度，block需要控制的原因。</p>
<p>2：为什么现在的L1 cache还在提高相关度？我认为最关键的一点是：现在基本所有的L1 cache都是使用的virtual index，physical tag的方案。这样方案带来的问题是，cache可用的位数基本固定为16位，因为一个page的大小是4Kb，所以offset大小固定为16位。提高相关度可以加载更多的数据进入缓存，提高缓存大小。这个方案可以在虚拟地址翻译成物理地址前就对缓存进行索引。是个非常有吸引力的方案。还能为多线程带来的conflict Miss带来好处</p>
## 第二种：使用路预测缩短命中时间
### 基本逻辑
<p>这个很好理解，可以理解成在多路复用器那里加一个简单的分支预测器。一般来说，这种方案不一定能优化命中时间，更可能优化功耗。</p>
## 第三种：cache流水线化，或者使用multiBank技术
### 基本逻辑
<p> 流水线化的好处很明显，就是能够提高时钟频率，提高指令吞吐率。代价是增加单个指令的处理时延。  </p>
<p> 这里简单介绍一下什么是cache技术里说到的bank（自己看书理解的，不一定准确）。就是例如我们在逻辑上有一个大小为64Kb的L1 cache，我们现在想超标量地处理缓存命令，或者节约功耗。我们就可以在物理意义上，将64Kb的缓存，分成四个小块（bank），这样就可以同时读取多个不同板块中的数据，达到超标量的目的。同时如果想节约功耗，可以只处理一条缓存命令，那么在单位时间内，有且仅有一个bank被使用，因此也可以节约功耗。
值得注意的是，这样的技术希望我们平均地读取每个bank中的内容，如果内容集中在某个bank里，那么这个方法就失去了意义，所以在地址的分配上需要作出改动。例如0-15,这16个cache line。一个经典的分类方法是『0,4,8,12』，『1,5,9,13』，『2,6,10,14』，『3,7,11,15』，这样的方法叫做顺序交错（sequential interleaving），一号bank的行号是4的倍数，2号是模4余1,以此类推。这样能降低bank之间的冲突。  </p>
